{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TM10007 group 14.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anoukzwinkels/TM10007/blob/master/TM10007_group_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcLbZjwLHuP_"
      },
      "source": [
        "# Predicting tumor stage in head and neck cancer with machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejYhOlkUHuQv"
      },
      "source": [
        "*Written by group 14: Merel Flinsenberg [4663179], Femke Leenen [4664485], Floor Smits [4533143] & Anouk Zwinkels [4679962]*\n",
        "\n",
        "---\n",
        "\n",
        "This code was written for the Machine Learning course of the master Technical Medicine from the Delft University of Technology. The aim of the code is to build a classifier that can predict the tumor stage in head and neck cancer based on chosen features. When succeeded, an biopsy or resection is no longer needed. Instead, a CT scan will be sufficient to determine the malignancy of the tumor. Today, due to manual assessment, quantification of the CT scans is still observer-dependent, subjective and difficult. Machine learning can solve this problem.\n",
        "\n",
        "Between the cells of code, the choices made are briefly explained and motivated. More details can be found in the report. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Amwa-J7HuQx"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "First, all data is loaded from github. After this, all the required packages are imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZHSX2VeHuQ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "44537221-aa93-4f23-9c7d-09f4be4d87d0"
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/anoukzwinkels/TM10007\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from hn.load_data               import load_data\n",
        "from sklearn.model_selection    import StratifiedKFold, RepeatedStratifiedKFold\n",
        "from sklearn.model_selection    import StratifiedShuffleSplit, RandomizedSearchCV\n",
        "from sklearn.feature_selection  import VarianceThreshold, RFECV, SelectKBest, f_classif\n",
        "from sklearn.metrics            import accuracy_score\n",
        "from sklearn.metrics            import roc_auc_score\n",
        "from sklearn.preprocessing      import RobustScaler\n",
        "from sklearn.neighbors          import KNeighborsClassifier\n",
        "from sklearn.svm                import SVC\n",
        "from sklearn.decomposition      import PCA\n",
        "\n",
        "from plotly.express             import scatter_3d, bar\n",
        "from sklearn.ensemble           import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-115a86414841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m               \u001b[0;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m    \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepeatedStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m    \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hn'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqxrQk8AHuRD"
      },
      "source": [
        "def def_data():\n",
        "  '''\n",
        "  Load the data\n",
        "  \n",
        "  :return features:     dataframe containing the features\n",
        "  :return labels:       dataframe containing the labels\n",
        "  '''\n",
        "  \n",
        "  # load the data\n",
        "  data = load_data()\n",
        "\n",
        "  # split the labels from the features\n",
        "  features = data.drop(columns=['label'])       # features\n",
        "  labels = pd.DataFrame(data['label'])          # labels\n",
        "\n",
        "  return features, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJiFUiXZp34s"
      },
      "source": [
        "def remove_outliers(X_train):\n",
        "  '''\n",
        "  Identify the outliers and replace them with the minimum or maximum value of the corresponding feature.\n",
        "\n",
        "  :param X_train:      dataframe containing the features of the traindata\n",
        "  :return X_train:     dataframe containing the features of the traindata with the outliers corrected\n",
        "  '''\n",
        "  # make a copy of X_train to fill in the corrected outliers\n",
        "  X_train = X_train.copy()\n",
        "\n",
        "  # calculate interquartile range\n",
        "  q25, q75 = np.percentile(X_train, 25,axis=0), np.percentile(X_train, 75,axis=0)\n",
        "  iqr = q75 - q25\n",
        "\n",
        "  # calculate the outlier cutoff\n",
        "  cut_off = iqr * 1.5\n",
        "  lower, upper = q25 - cut_off, q75 + cut_off\n",
        "\n",
        "  # identify the outliers\n",
        "  outliers = (X_train < lower) | (X_train > upper)\n",
        "\n",
        "  # make a dataframe without the outliers. This is needed to calculate the min and max value.\n",
        "  feature_nan = X_train[:].copy()\n",
        "  feature_nan[outliers] = np.nan\n",
        "\n",
        "  # make a list of the feature names. This is needed to isolate the column, as the outliers are corrected separately for each column. \n",
        "  feature_names = list(X_train.columns)\n",
        "  for col in feature_names:\n",
        "    # isolate the column\n",
        "    outliers_col = outliers[col]\n",
        "    # get the value (val) of the outlier\n",
        "    for val in X_train.loc[outliers_col,col]:\n",
        "      # if the outlier is greater than the max value (non-outliers), it is replaced by this max value.\n",
        "      if val > np.nanmax(feature_nan[col]):\n",
        "        X_train[col] = X_train[col].replace(to_replace = val, value = np.nanmax(feature_nan[col]))\n",
        "      # if the outlier is less than the min value (non-outliers), it is replaced by this min value.\n",
        "      elif val < np.nanmin(feature_nan[col]):\n",
        "        X_train[col] = X_train[col].replace(to_replace = val, value = np.nanmin(feature_nan[col]))\n",
        "        \n",
        "  return (X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DilepvU4cEbX"
      },
      "source": [
        "# Feature selection\n",
        "\n",
        "To choose which features will be used in the classification, feature selection is performed. Two different methods of feature selection are used together to achieve the best features. The two methods and why they were used will be briefly explained separately prior to the corresponding code of the method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sW1TDc3C3p7"
      },
      "source": [
        "def remove_zero_variance(X_train, X_test):\n",
        "  '''\n",
        "  Remove the features with zero variance. The data is fitted on the X_train and applied to X_train and X_test.\n",
        "\n",
        "  :param X_train:     dataframe containing the features of the traindata           \n",
        "  :param X_test:      dataframe containing the features of the testdata         \n",
        "  :return X_train:    dataframe containing the features of the traindata with the features with zero variance removed\n",
        "  :return X_test:     dataframe containing the features of the testdata with the features with zero variance removed\n",
        "  '''\n",
        "\n",
        "  # make a model to filter out all the features with zero variance\n",
        "  m = VarianceThreshold(threshold=0.0)\n",
        "  # fit the model on the train data\n",
        "  m.fit(X_train)\n",
        "  \n",
        "  # get the boolean of the fitted data. This way the column names are staying intact.\n",
        "  names_chosen_features = m.get_support(indices = True)\n",
        "\n",
        "  # apply the model on the train data\n",
        "  X_train = X_train.iloc[:,names_chosen_features]\n",
        "  # apply the model on the test data\n",
        "  X_test = X_test.iloc[:,names_chosen_features]\n",
        "\n",
        "  return X_train, X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEN6Qh8bTWTm"
      },
      "source": [
        "def select_features(X_train, X_test, number = 20):\n",
        "  '''\n",
        "  Select the best features. \n",
        "\n",
        "  :param X_train:         dataframe containing features of the the traindata           \n",
        "  :param X_test:          dataframe containing the features of the testdata   \n",
        "  :param number:          number of features to be selected  \n",
        "  :return X_train:        dataframe containing the features of the traindata with only the selected features\n",
        "  :return X_test:         dataframe containing the features of the testdata with only the selected features\n",
        "  :return feature_names:  array containing the chosen feature names\n",
        "  '''\n",
        "\n",
        "  Y=np.ravel(y_train)\n",
        "\n",
        "  # make a model for the SelectKBest \n",
        "  m = SelectKBest(f_classif, number)\n",
        "  # fit the model on the train data\n",
        "  m.fit(X_train, Y)\n",
        "\n",
        "  # get the boolean of the fitted data. This way the column names are staying intact.\n",
        "  names_chosen_features = m.get_support(indices = True)\n",
        "\n",
        "  # apply the model on the train data\n",
        "  X_train = X_train.iloc[:,names_chosen_features]\n",
        "\n",
        "  # apply the model on the test data\n",
        "  X_test = X_test.iloc[:,names_chosen_features]\n",
        "\n",
        "  # extract the feature names\n",
        "  feature_names = X_train.columns\n",
        "  \n",
        "  return X_train, X_test, feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7QhjrAyH77F"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D6808ra57yL"
      },
      "source": [
        "def scaling(X_train, X_test):\n",
        "  '''\n",
        "  To scale the test and train data.\n",
        "\n",
        "  :param X_train:         dataframe containing the features of the traindata           \n",
        "  :param X_test:          dataframe containing the features of the testdata               \n",
        "  :return X_train:        dataframe containing the scaled features of the traindata\n",
        "  :return X_test:         dataframe containing the scaled features of the testdata\n",
        "  '''\n",
        "\n",
        "  # make a model for the robust scaler\n",
        "  scaler = RobustScaler()\n",
        "\n",
        "  # fit the model on the train data\n",
        "  scaler.fit(X_train)\n",
        "\n",
        "  # apply the model on the train data\n",
        "  X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
        "  # apply the model on the test data\n",
        "  X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
        "  \n",
        "  return X_train, X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x77nLzx0DikN"
      },
      "source": [
        "# Random Forest\n",
        "\n",
        "def hyperrandfor(X_train, y_train):\n",
        "  '''\n",
        "  To fit the hyperparameters of the random-forest\n",
        "\n",
        "  :param X_train:         dataframe containing the features of the train data           \n",
        "  :param X_test:          dataframe containing the features of the test data              \n",
        "  :return fitted_clfs:    list containing the fitted Random Forest classifier on the train data\n",
        "  '''\n",
        "  \n",
        "  n_estimators = list(range(64, 128)) \n",
        "  fitted_clfs = list()\n",
        "  # Convert to dictionary\n",
        "  hyperparameters = dict(n_estimators=n_estimators)\n",
        "  # Create new KNN object\n",
        "  rf_clf = RandomForestClassifier()\n",
        "  # Use GridSearch\n",
        "  clf = RandomizedSearchCV(rf_clf, hyperparameters, cv=10, n_iter=30)\n",
        "  # Fit the classifier\n",
        "  clf.fit(X_train, y_train)  \n",
        "  # Save for next part\n",
        "  fitted_clfs.append(clf)\n",
        "\n",
        "  return fitted_clfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0Bu1OWiS7zX"
      },
      "source": [
        "# Best estimators random forest \n",
        "\n",
        "def best_est_randf():\n",
        "  '''\n",
        "  To determine the optimal values for the hyperparameters for the random forest.\n",
        "         \n",
        "  :return best_n_estimator: best value for hyperparameter n_estimators \n",
        "  '''\n",
        "  \n",
        "  fitted_clfs = hyperrandfor(X_train, y_train)\n",
        "  # Get the best estimator and best parameters belonging to that estimator\n",
        "  for num, clf in enumerate(fitted_clfs):\n",
        "      best_n_estimators = clf.best_estimator_.get_params()['n_estimators']\n",
        "  return best_n_estimators "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiszc5UBF1-p"
      },
      "source": [
        "# K-nearest neighbor\n",
        "\n",
        "def hyperknear(X_train, y_train):\n",
        "  '''\n",
        "  To fit the hyperparameters of the k-nearest neighbor\n",
        "\n",
        "  :param X_train:             dataframe containing the features of the traindata           \n",
        "  :param X_test:              dataframe containing the features of the testdata               \n",
        "  :return fitted_clfs_knear:  list containing the fitted K-nearest neighbor classifier on the train data\n",
        "  '''\n",
        "\n",
        "  n_neighbors = list(range(3,20,2))\n",
        "  p=[1,2]\n",
        "  weights = ['uniform', 'distance']\n",
        "  fitted_clfs_knear = list()\n",
        "  # Convert to dictionary\n",
        "  hyperparameters = dict(n_neighbors=n_neighbors, p=p, weights=weights)\n",
        "  # Create new KNN object\n",
        "  knn_2 = KNeighborsClassifier()\n",
        "  # Use GridSearch\n",
        "  clf = RandomizedSearchCV(knn_2, hyperparameters, cv=10, n_iter=30)\n",
        "  # Fit the model\n",
        "  clf.fit(X_train, y_train)\n",
        "  fitted_clfs_knear.append(clf)\n",
        "\n",
        "  return fitted_clfs_knear"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep6K4fEuV1Lb"
      },
      "source": [
        "# Best estimators/parameters k nearest neighbor\n",
        "\n",
        "def best_est_knear():\n",
        "  '''\n",
        "  To determine the optimal values for the hyperparameters for the k-Nearest Neighbor.\n",
        "\n",
        "  :return best_p:           best value for hyperparameter p\n",
        "  :return best_n_neighbors: best value for hyperparameter n_neighbors\n",
        "  :return best_weights:     best weight function for hyperparameter weights\n",
        "  '''\n",
        "  \n",
        "  fitted_clfs_knear = hyperknear(X_train, y_train)\n",
        "# Get the best estimator and best parameters belonging to that estimator\n",
        "  for num, clf in enumerate(fitted_clfs_knear):\n",
        "      best_p = clf.best_estimator_.get_params()['p']\n",
        "      best_n_neighbors = clf.best_estimator_.get_params()['n_neighbors']\n",
        "      best_weights = clf.best_estimator_.get_params()['weights']\n",
        "  return best_p, best_n_neighbors, best_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SEi2vvMHVAU"
      },
      "source": [
        " # SVM\n",
        "\n",
        "def hypersvm(X_train, y_train):\n",
        "  '''\n",
        "  To fit the hyperparameters of the Support Vector Machine\n",
        "\n",
        "  :param X_train:            dataframe containing the features of the traindata           \n",
        "  :param y_train:            dataframe containing the labels of the traindata        \n",
        "  :return fitted_clfs_svm:   list containing the fitted SVM classifier on the train data\n",
        "  '''\n",
        "  \n",
        "  C = [0.01, 0.5, 1]\n",
        "  kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "  degree = [2, 3, 4, 5]\n",
        "  fitted_clfs_svm = list()\n",
        "  # Convert to dictionary\n",
        "  hyperparameters = dict(C=C, kernel=kernel, degree=degree)\n",
        "  # Create new KNN object\n",
        "  svm_clf = SVC()\n",
        "  # Use RandomizedSearch\n",
        "  clf = RandomizedSearchCV(svm_clf, hyperparameters, cv=10, n_iter=30)\n",
        "  # Fit the model\n",
        "  clf.fit(X_train, y_train)\n",
        "  fitted_clfs_svm.append(clf)\n",
        "\n",
        "  return fitted_clfs_svm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbxYLQCNbGza"
      },
      "source": [
        "# Best estimators/parameters SVM\n",
        "\n",
        "def best_est_svm():\n",
        "  '''\n",
        "  To determine the optimal values for the hyperparameters for the Support Vector Machine.\n",
        "\n",
        "  :return best_c:       best value for hyperparameter C\n",
        "  :return best_kernel:  best kernel for hyperparameter kernel \n",
        "  :return best_degree:  best value for hyperparameter when 'poly' kernel is used \n",
        "  '''\n",
        "  \n",
        "  fitted_clfs_svm = hypersvm(X_train, y_train)\n",
        "  \n",
        "  # Get the best estimator and best parameters belonging to that estimator\n",
        "  for num, clf in enumerate(fitted_clfs_svm):\n",
        "      best_c = clf.best_estimator_.get_params()['C']\n",
        "      best_kernel = clf.best_estimator_.get_params()['kernel']\n",
        "      best_degree = clf.best_estimator_.get_params()['degree']\n",
        "  return best_c, best_kernel, best_degree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5djxC8mTKLS"
      },
      "source": [
        " from sklearn.metrics  import confusion_matrix\n",
        " def evaluation(X_train, y_train, X_test, y_test, clf):\n",
        "   '''\n",
        "   For evaluation of the classifier, the accuracy, AUC, specificity and sensitivity\n",
        "   need to be determined. \n",
        "\n",
        "   :param X_train: dataframe containing the features of the train data           \n",
        "   :param y_train: dataframe containing the labels of the train data\n",
        "   :param X_test:  dataframe containing the features of the test data\n",
        "   :param y_test:  dataframe containing the labels of the test data\n",
        "   :param clf:     used classifier \n",
        "   :return eval:   list contraining the values for accuracy, AUC, sensitivity \n",
        "                   and specificity for the used classifier\n",
        "   '''\n",
        "   clf.fit(X_train, y_train)\n",
        "   y_pred = clf.predict(X_test)\n",
        "   accuracy = accuracy_score(y_test, y_pred)\n",
        "   matrix = confusion_matrix(y_test, y_pred)\n",
        "   sens = matrix[0,0] / (matrix[0,0]+matrix[0,1])\n",
        "   spec = matrix[1,1] / (matrix[1,0]+matrix[1,1])\n",
        "   y_pred_auc = clf.predict_proba(X_test)[:,1]\n",
        "   auc = roc_auc_score(y_test, y_pred_auc)\n",
        "   eval = [accuracy, auc, sens, spec]\n",
        "   return eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLLr7ad5t6K0"
      },
      "source": [
        "def constrclassandacc(X_train, y_train, X_test, y_test, cv_ind):\n",
        "  '''\n",
        "  Function to construct classifiers with optimal hyperparameters and \n",
        "\n",
        "  :param X_train:         dataframe containing the features of the train data \n",
        "  :param Y_train:         dataframe containing the labels of the train data     \n",
        "  :param X_test:          dataframe containing the features of the test data  \n",
        "  :param y_test:          dataframe containing the labels of the test data\n",
        "  :param cv_ind:          number of the cross-validation split that is running\n",
        "  :return svm_df:         dataframe containing, per cross validation iteration,\n",
        "                          the best kernel, degree and C for SVM classifier and \n",
        "                          the accuracy of this classifier on the test set \n",
        "  :return knn_df:         dataframe containing, per cross validation iteration,\n",
        "                          the best P, n_neighbors and weights for the KNN \n",
        "                          classifier and the accuracy of this classifier on the\n",
        "                          test set\n",
        "  :return rfc_df:         dataframe containing, per cross validation iteration,\n",
        "                          the best n_estimators for the Random Forest Classifier\n",
        "                          and the accuracy of this classifier on the test set\n",
        "  :return acc_df:         dataframe containing, per cross validation iteration,\n",
        "                          the accuracies per classifier and the best accuracy\n",
        "  :return auc_df:         dataframe containing, per cross validation iteration,\n",
        "                          the AUCs per classifier and the best accuracy\n",
        "  :return sens_df:        dataframe containing, per cross validation iteration,\n",
        "                          the sensitivities per classifier and the best accuracy\n",
        "  :return spec_df:        dataframe containing, per cross validation iteration,\n",
        "                          the specificities per classifier and the best accuracy\n",
        "  '''\n",
        "  \n",
        "  best_n_estimators = best_est_randf()\n",
        "  best_p, best_n_neighbors, best_weights = best_est_knear()\n",
        "  best_c, best_kernel, best_degree = best_est_svm()\n",
        "\n",
        "  SVM = SVC(kernel=best_kernel, degree=best_degree, C=best_c, probability=True)\n",
        "  KNN = KNeighborsClassifier(p=best_p, n_neighbors=best_n_neighbors, weights=best_weights)\n",
        "  RFC = RandomForestClassifier(n_estimators=best_n_estimators)\n",
        "\n",
        "  svm_df = pd.DataFrame()\n",
        "  knn_df = pd.DataFrame()\n",
        "  rfc_df = pd.DataFrame()\n",
        "  acc_df = pd.DataFrame()\n",
        "  auc_df = pd.DataFrame()\n",
        "  sens_df = pd.DataFrame()\n",
        "  spec_df = pd.DataFrame()\n",
        "  \n",
        "  eval_SVM = evaluation(X_train, y_train, X_test, y_test, SVM)\n",
        "  eval_KNN = evaluation(X_train, y_train, X_test, y_test, KNN)\n",
        "  eval_RFC = evaluation(X_train, y_train, X_test, y_test, RFC)\n",
        "\n",
        "  svm_df.loc[cv_ind, 0] = best_kernel\n",
        "  svm_df.loc[cv_ind, 1] = best_degree\n",
        "  svm_df.loc[cv_ind, 2] = best_c\n",
        "  svm_df.loc[cv_ind, 3] = eval_SVM[1]\n",
        "\n",
        "  knn_df.loc[cv_ind, 0] = best_p\n",
        "  knn_df.loc[cv_ind, 1] = best_n_neighbors\n",
        "  knn_df.loc[cv_ind, 2] = best_weights\n",
        "  knn_df.loc[cv_ind, 3] = eval_KNN[1]\n",
        "\n",
        "  rfc_df.loc[cv_ind, 0] = best_n_estimators\n",
        "  rfc_df.loc[cv_ind, 1] = eval_RFC[1]\n",
        "\n",
        "  acc_df.loc[cv_ind, 0] = eval_SVM[0]\n",
        "  acc_df.loc[cv_ind, 1] = eval_KNN[0]\n",
        "  acc_df.loc[cv_ind, 2] = eval_RFC[0]\n",
        "\n",
        "  auc_df.loc[cv_ind, 0] = eval_SVM[1]\n",
        "  auc_df.loc[cv_ind, 1] = eval_KNN[1]\n",
        "  auc_df.loc[cv_ind, 2] = eval_RFC[1]\n",
        "\n",
        "  sens_df.loc[cv_ind, 0] = eval_SVM[2]\n",
        "  sens_df.loc[cv_ind, 1] = eval_KNN[2]\n",
        "  sens_df.loc[cv_ind, 2] = eval_RFC[2]\n",
        "\n",
        "  spec_df.loc[cv_ind, 0] = eval_SVM[3]\n",
        "  spec_df.loc[cv_ind, 1] = eval_KNN[3]\n",
        "  spec_df.loc[cv_ind, 2] = eval_RFC[3]\n",
        "\n",
        "  return svm_df, knn_df, rfc_df, acc_df, auc_df, sens_df, spec_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI1HR65eqOKv"
      },
      "source": [
        "def printinghyperpar(SVM, KNN, RF):\n",
        "  ''' \n",
        "  Function to print the hyperparameters that were used for the different \n",
        "  classifiers per cross-validation iteration of the test data. \n",
        "\n",
        "  :param SVM: the SVM classifier with the trained hyperparameters \n",
        "  :param KNN: the KNN classifier with the trained hyperparameters\n",
        "  :param RF:  the Random Forest classifier with the trained hyperparameters \n",
        "  '''\n",
        "\n",
        "  print('Support Vector Machine')\n",
        "  print('Hyperparameters')\n",
        "  print(SVM)\n",
        "  print('\\n')\n",
        "\n",
        "  print('k-Nearest Neighbors')\n",
        "  print('Hyperparameters')\n",
        "  print(KNN)\n",
        "  print('\\n')\n",
        "\n",
        "  print('Random Forest')\n",
        "  print('Hyperparameters')\n",
        "  print(RF)\n",
        "  print('\\n')\n",
        "\n",
        "def printingeval(acc, auc, sens, spec):\n",
        "  '''\n",
        "  To print all scoring results and the mean score, maximum and minimum.\n",
        "  \n",
        "  :param acc:  accuracy/accuracies for the used classifiers on the test data\n",
        "  :param auc:  AUC(s) for the used classifiers on the test data\n",
        "  :param sens: sensitivity/sensitivities for the used classifiers on the test \n",
        "               data\n",
        "  :param spec: specificity/specificities for the used classifiers on the test \n",
        "               data\n",
        "  '''\n",
        "\n",
        "  print('Accuracy of all the classifiers')\n",
        "  print('Results'+'\\n'+'-'*80)\n",
        "  print(acc)\n",
        "  print('='*80)\n",
        "  print('Area Under the Curve of all the classifiers')\n",
        "  print('Results'+'\\n'+'-'*80)\n",
        "  print(auc)\n",
        "  print('='*80)\n",
        "  print('Sensitivity of all the classifiers')\n",
        "  print('Results'+'\\n'+'-'*80)\n",
        "  print(sens)\n",
        "  print('='*80)\n",
        "  print('Specificity of all the classifiers')\n",
        "  print('Results'+'\\n'+'-'*80)\n",
        "  print(spec)\n",
        "  print('='*80)\n",
        "  print('Mean accuracy:', acc['Optimal acc'].mean(), \n",
        "        '(min:', acc['Optimal acc'].min(), 'max:', \n",
        "        acc['Optimal acc'].max(), ' )')\n",
        "  print('Mean AUC:', auc['Max AUC'].mean(), \n",
        "        '(min:', auc['Max AUC'].min(), 'max:', \n",
        "        auc['Max AUC'].max(), ' )')\n",
        "  print('Mean sensitivity:', sens['Optimal sens'].mean(), \n",
        "        '(min:', sens['Optimal sens'].min(), 'max:', \n",
        "        sens['Optimal sens'].max(), ' )')\n",
        "  print('Mean specificity:', spec['Optimal spec'].mean(), \n",
        "        '(min:', spec['Optimal spec'].min(), 'max:', \n",
        "        spec['Optimal spec'].max(), ' )')\n",
        "  print('='*80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdUtlJkuaOHK"
      },
      "source": [
        "# run one time:\n",
        "features, labels = def_data()\n",
        "\n",
        "# In the train test loop\n",
        "sss = StratifiedShuffleSplit(n_splits=10)\n",
        "\n",
        "svm_df_all = pd.DataFrame()\n",
        "knn_df_all = pd.DataFrame()\n",
        "rfc_df_all = pd.DataFrame()\n",
        "acc_df_all = pd.DataFrame()\n",
        "auc_df_all = pd.DataFrame()\n",
        "sens_df_all = pd.DataFrame()\n",
        "spec_df_all = pd.DataFrame()\n",
        "features_df = pd.DataFrame()\n",
        "\n",
        "cv_ind = 0\n",
        "for train_index, test_index in sss.split(features, labels):\n",
        "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
        "    y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
        "    y_train = np.ravel(y_train)\n",
        "    \n",
        "    X_train = remove_outliers(X_train)\n",
        "    X_train, X_test = scaling(X_train, X_test)\n",
        "    X_train, X_test = remove_zero_variance(X_train, X_test)\n",
        "    X_train, X_test, feature_names = select_features(X_train, X_test)\n",
        "    run = cv_ind + 1\n",
        "    features_df['Run {}'.format(run)] = feature_names\n",
        "    \n",
        "    svm_df, knn_df, rfc_df, acc_df, auc_df, sens_df, spec_df = constrclassandacc(X_train, y_train, X_test, y_test, cv_ind)\n",
        "    svm_df_all = svm_df_all.append(svm_df)\n",
        "    knn_df_all = knn_df_all.append(knn_df)\n",
        "    rfc_df_all = rfc_df_all.append(rfc_df)\n",
        "    acc_df_all = acc_df_all.append(acc_df)\n",
        "    auc_df_all = auc_df_all.append(auc_df)\n",
        "    sens_df_all = sens_df_all.append(sens_df)\n",
        "    spec_df_all = spec_df_all.append(spec_df)\n",
        "\n",
        "    cv_ind += 1\n",
        "\n",
        "auc_df_all.columns = ['AUC SVM', 'AUC KNN', 'AUC RFC']\n",
        "auc_df_all['Max AUC'] = auc_df_all.max(axis=1)    \n",
        "max_value_index = auc_df_all.idxmax(axis = 1)\n",
        "\n",
        "svm_df_all.columns = ['Kernel', 'Degree', 'C', 'AUC']\n",
        "knn_df_all.columns = ['P', 'N Neighbors', 'Weights', 'AUC']\n",
        "rfc_df_all.columns = ['N estimators', 'AUC']\n",
        "acc_df_all.columns = ['Acc SVM', 'Acc KNN', 'Acc RFC']\n",
        "sens_df_all.columns = ['Sens SVM', 'Sens KNN', 'Sens RFC']\n",
        "spec_df_all.columns = ['Spec SVM', 'Spec KNN', 'Spec RFC']\n",
        "\n",
        "acc_df_all['Optimal acc'] = list(range(10))\n",
        "sens_df_all['Optimal sens'] = list(range(10))\n",
        "spec_df_all['Optimal spec'] = list(range(10))\n",
        "\n",
        "auc_df_all2 = auc_df_all.to_numpy()\n",
        "max_value_index = auc_df_all2.argmax(axis=1) \n",
        "\n",
        "for index, value in enumerate(max_value_index):\n",
        "  acc_df_all.iloc[index,3] = acc_df_all.iloc[index, value]\n",
        "  sens_df_all.iloc[index,3] = sens_df_all.iloc[index, value]\n",
        "  spec_df_all.iloc[index,3] = spec_df_all.iloc[index, value]\n",
        "\n",
        "acc_df_all\n",
        "\n",
        "printingeval(acc_df_all, auc_df_all, sens_df_all, spec_df_all)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JA-YJDBWI6i"
      },
      "source": [
        "#auc_df_all = auc_df_all.to_numpy()  \n",
        "max_value_index = auc_df_all2.argmax(axis=1) \n",
        "\n",
        "max_value_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaSwq5Of-KLA"
      },
      "source": [
        "# find the column name of maximum\n",
        "# values in every row\n",
        "maxValueIndex = auc_df_all.idxmax(axis = 1)\n",
        "maxValueIndex = maxValueIndex\n",
        "print(\"Max values of row are at following columns :\")\n",
        "print(maxValueIndex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qw7IK4-nLUW"
      },
      "source": [
        "pd.DataFrame.to_latex(acc_df_all,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTHEt8_3nZ9U"
      },
      "source": [
        "pd.DataFrame.to_latex(auc_df_all,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqq1pVnqnaX1"
      },
      "source": [
        "pd.DataFrame.to_latex(sens_df_all,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37xrml8fna2H"
      },
      "source": [
        "pd.DataFrame.to_latex(spec_df_all,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ghKaP5DncEB"
      },
      "source": [
        "pd.DataFrame.to_latex(features_df.iloc[:,9],index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad9ufnZinayX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l980Y5ZyQpJC"
      },
      "source": [
        "features_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A6Krs73fbiy"
      },
      "source": [
        "knn_df_all"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}